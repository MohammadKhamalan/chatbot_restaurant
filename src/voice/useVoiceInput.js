export async function startVoiceCapture(onFinalText, sessionId, sttLang = "ar-SA") {
  // Browser Speech Recognition ONLY (simple & stable)
  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

  if (!SpeechRecognition) {
    const errorMsg = "Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªØµÙØ­ Chrome Ø£Ùˆ Edge.";
    alert(errorMsg);
    throw new Error("SpeechRecognition not supported");
  }

  // Check if HTTPS (required for getUserMedia on mobile)
  if (window.location.protocol !== 'https:' && window.location.hostname !== 'localhost' && window.location.hostname !== '127.0.0.1') {
    const errorMsg = "Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙŠØªØ·Ù„Ø¨ HTTPS. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§ØªØµØ§Ù„ Ø¢Ù…Ù†.";
    alert(errorMsg);
    throw new Error("HTTPS required");
  }

  // Stop any previous
  await stopVoiceCapture();

  const recognition = new SpeechRecognition();
  recognition.lang = sttLang; // âœ… ar-SA by default
  recognition.interimResults = false;
  recognition.continuous = false;
  recognition.maxAlternatives = 1;

  window.__voiceRecognition = recognition;

  // Store callback for error handling
  let errorCallback = null;

  recognition.onresult = async (event) => {
    console.log("ğŸ¤ Speech recognition result received:", event);
    
    // Handle both single and multiple results
    let transcript = "";
    if (event.results && event.results.length > 0) {
      // Get the most confident result
      const result = event.results[event.results.length - 1];
      if (result && result.length > 0) {
        transcript = result[0].transcript || "";
      }
    }
    
    const finalText = transcript.trim();
    console.log("ğŸ¤ Final transcript:", finalText);

    if (finalText) {
      try {
        await onFinalText(finalText);
      } catch (error) {
        console.error("Error in onFinalText callback:", error);
      }
    } else {
      console.warn("âš ï¸ Empty transcript received");
    }
  };

  recognition.onerror = (e) => {
    console.error("âŒ Speech recognition error:", e.error);
    console.error("âŒ Error details:", {
      error: e.error,
      message: e.message,
      userAgent: navigator.userAgent,
      protocol: window.location.protocol,
      hostname: window.location.hostname,
    });
    
    // Handle specific error cases
    let errorMessage = "";
    switch (e.error) {
      case "not-allowed":
      case "permission-denied":
        errorMessage = "ØªÙ… Ø±ÙØ¶ Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†. Ø§Ù„Ø±Ø¬Ø§Ø¡:\n1. Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ØªØµÙØ­\n2. Ø¥ØºÙ„Ø§Ù‚ Ø£ÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø£Ø®Ø±Ù‰ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†\n3. Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰";
        break;
      case "no-speech":
        // This is normal, user didn't speak - don't show error
        console.log("â„¹ï¸ No speech detected (normal)");
        return;
      case "aborted":
        // User stopped - don't show error
        console.log("â„¹ï¸ Speech recognition aborted (normal)");
        return;
      case "network":
        errorMessage = "Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ©. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„ ÙˆØ§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.";
        break;
      case "service-not-allowed":
        errorMessage = "Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª ØºÙŠØ± Ù…ØªØ§Ø­Ø©. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹.";
        break;
      case "audio-capture":
        errorMessage = "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¬Ù‡Ø§Ø².";
        break;
      default:
        errorMessage = `Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª: ${e.error}. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.`;
    }
    
    if (errorMessage) {
      // Only show error for critical issues
      if (e.error === "not-allowed" || e.error === "permission-denied" || e.error === "network" || e.error === "audio-capture") {
        alert(errorMessage);
      }
    }
  };

  recognition.onstart = () => {
    console.log("Speech recognition started");
  };

  recognition.onend = () => {
    // no auto-restart (clean)
    console.log("Speech recognition ended");
    // On mobile, sometimes onend fires without onresult
    // This is normal if user didn't speak or recognition timed out
    // Clean up the recognition object
    if (window.__voiceRecognition === recognition) {
      window.__voiceRecognition = null;
    }
  };

  try {
    recognition.start();
  } catch (error) {
    console.error("Failed to start recognition:", error);
    
    // If permission is denied, show helpful message
    if (error.name === "NotAllowedError" || error.message.includes("permission")) {
      alert("Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù…Ø·Ù„ÙˆØ¨. Ø§Ù„Ø±Ø¬Ø§Ø¡:\n1. Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø£ÙŠÙ‚ÙˆÙ†Ø© Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù…Ø±Ø© Ø£Ø®Ø±Ù‰\n2. Ø§Ø³Ù…Ø­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ø¹Ù†Ø¯ Ø§Ù„Ø·Ù„Ø¨\n3. ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø£Ø®Ø±Ù‰ Ù„Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†");
    } else {
      alert(`ÙØ´Ù„ Ø¨Ø¯Ø¡ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª: ${error.message}. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.`);
    }
    throw error;
  }

  return recognition;
}

export async function stopVoiceCapture() {
  if (window.__voiceRecognition) {
    try {
      window.__voiceRecognition.stop();
    } catch (e) {
      // ignore
    }
    window.__voiceRecognition = null;
  }
}